# ğŸ“¥ Guide de Scraping - ETL Pipeline

## ğŸ¯ Objectif

Ce guide explique comment rÃ©cupÃ©rer les donnÃ©es depuis diffÃ©rentes sources et les sauvegarder en format JSON/CSV.

---

## ğŸ“š Sources de donnÃ©es

### 1. ExerciseDB (GitHub) âœ… PrÃªt
- **Source** : [Free Exercise DB](https://github.com/yuhonas/free-exercise-db)
- **Format** : JSON via API
- **Authentification** : Aucune
- **Scraper** : `src/scrapers/exercisedb_scraper.py`

### 2. Datasets Kaggle ğŸ”‘ NÃ©cessite configuration
- **Sources** :
  - Daily Food & Nutrition Dataset
  - Diet Recommendations Dataset
  - Gym Members Exercise Dataset
  - Fitness Tracker Dataset
- **Format** : CSV (tÃ©lÃ©chargement puis extraction)
- **Authentification** : API Kaggle requise
- **Scraper** : `src/scrapers/kaggle_scraper.py`

---

## ğŸš€ Utilisation

### Option 1 : Scraper ExerciseDB (Simple - Aucune config requise)

```bash
# Activer l'environnement virtuel
source venv/Scripts/activate

# ExÃ©cuter le scraper ExerciseDB
python -m src.scrapers.exercisedb_scraper
```

**RÃ©sultat** : Fichier JSON crÃ©Ã© dans `data/raw/exercisedb_raw_YYYYMMDD_HHMMSS.json`

### Option 2 : TÃ©lÃ©charger depuis Kaggle (NÃ©cessite configuration)

#### Ã‰tape 1 : Configuration Kaggle API

1. **CrÃ©er un compte Kaggle** : [kaggle.com](https://www.kaggle.com)

2. **Obtenir votre token API** :
   - Aller sur : https://www.kaggle.com/account
   - Cliquer sur "Create New API Token"
   - Un fichier `kaggle.json` sera tÃ©lÃ©chargÃ©

3. **Placer le token** :
   ```bash
   # Windows
   mkdir %USERPROFILE%\.kaggle
   move kaggle.json %USERPROFILE%\.kaggle\
   
   # Linux/Mac
   mkdir -p ~/.kaggle
   mv kaggle.json ~/.kaggle/
   chmod 600 ~/.kaggle/kaggle.json
   ```

4. **Installer Kaggle CLI** :
   ```bash
   pip install kaggle
   ```

#### Ã‰tape 2 : TÃ©lÃ©charger les datasets

```bash
# TÃ©lÃ©charger un seul dataset
python -m src.scrapers.kaggle_scraper

# Ou tÃ©lÃ©charger tous les datasets via le pipeline complet
python -m src.scrapers.run_scraping
```

### Option 3 : Pipeline complet

```bash
python -m src.scrapers.run_scraping
```

Cette commande exÃ©cute tous les scrapers sÃ©quentiellement.

---

## ğŸ“‚ Structure des donnÃ©es rÃ©cupÃ©rÃ©es

### ExerciseDB
```
data/raw/exercisedb_raw_YYYYMMDD_HHMMSS.json
{
  "metadata": {
    "source": "ExerciseDB",
    "total_exercises": 800+,
    "categories": {...},
    "scraped_at": "2026-01-09 14:30:00"
  },
  "exercises": [
    {
      "name": "3/4 Sit-Up",
      "category": "strength",
      "equipment": "body only",
      "primaryMuscles": ["abdominals"],
      "images": ["url1", "url2"],
      ...
    }
  ]
}
```

### Kaggle Datasets
```
data/raw/kaggle/
â”œâ”€â”€ daily-food-and-nutrition-dataset/
â”‚   â””â”€â”€ *.csv
â”œâ”€â”€ diet-recommendations-dataset/
â”‚   â””â”€â”€ *.csv
â”œâ”€â”€ gym-members-exercise-dataset/
â”‚   â””â”€â”€ *.csv
â””â”€â”€ fitness-tracker-dataset/
    â””â”€â”€ *.csv
```

---

## ğŸ” VÃ©rification des donnÃ©es

### Voir les logs

```bash
cat data/logs/etl.log
```

### Lister les fichiers tÃ©lÃ©chargÃ©s

```bash
ls -R data/raw/
```

---

## âš ï¸ RÃ©solution de problÃ¨mes

### Erreur : Kaggle API not found

```bash
pip install kaggle
```

### Erreur : 401 Unauthorized (Kaggle)

VÃ©rifiez que `kaggle.json` est bien placÃ© dans `~/.kaggle/` et contient vos credentials.

### Erreur : Module not found

```bash
# S'assurer que l'environnement virtuel est activÃ©
source venv/Scripts/activate

# RÃ©installer les dÃ©pendances
pip install -r requirements.txt
```

### Erreur : Connection timeout

Augmentez le timeout dans `.env` :
```
TIMEOUT=60
```

---

## ğŸ“Š Prochaines Ã©tapes

Une fois les donnÃ©es rÃ©cupÃ©rÃ©es :
1. âœ… VÃ©rifier l'intÃ©gritÃ© des fichiers
2. ğŸ§¹ Nettoyer les donnÃ©es avec les processors
3. ğŸ’¾ Charger dans la base de donnÃ©es PostgreSQL
4. ğŸ“ˆ CrÃ©er les visualisations

---

## ğŸ› ï¸ Personnalisation

### Ajouter une nouvelle source

1. CrÃ©er un nouveau scraper dans `src/scrapers/`
2. HÃ©riter de la classe de base ou crÃ©er une nouvelle classe
3. ImplÃ©menter les mÃ©thodes `fetch()` et `save()`
4. Ajouter au pipeline dans `run_scraping.py`

### Exemple de structure :

```python
from src.utils.logger import setup_logger
from src.utils.file_handler import save_to_json

class MyCustomScraper:
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
    
    def fetch_data(self):
        # Votre logique de rÃ©cupÃ©ration
        pass
    
    def run(self):
        data = self.fetch_data()
        save_to_json(data, "output.json")
        return data
```

---

**Document crÃ©Ã© pour : EPSI MSPR - ETL Pipeline**  
**DerniÃ¨re mise Ã  jour : 9 janvier 2026**
